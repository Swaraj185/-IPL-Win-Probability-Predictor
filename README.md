# ğŸ IPL Win Probability Predictor

**Name:** IPL Win Probability Predictor

**Description:** Predict realâ€‘time IPL win probabilities from current match context using a calibrated scikitâ€‘learn model and Streamlit UI.

## ğŸš€ Features

- **Real-time predictions** from match context
- **Interactive Streamlit UI** for inputs and results
- **Model selection** over multiple algorithms with crossâ€‘validation
- **Reproducible training** from public IPL datasets

## ğŸ› ï¸ Tech Stack

- Python, Streamlit, Pandas, NumPy, scikitâ€‘learn
- Model serialization via Pickle

## ğŸ“‚ Data

- `matches.csv`: Matchâ€‘level metadata (winner, city, etc.)
- `deliveries.csv`: Ballâ€‘byâ€‘ball data (runs, wickets, over/ball, teams)

The training script builds secondâ€‘innings, perâ€‘ball states to learn the probability of the chasing side winning from that state.

## ğŸ“Š Features Used

- `batting_team`, `bowling_team`, `city`
- `runs_left`, `balls_left`, `wickets`
- `total_runs_x` (target), `crr` (current run rate), `rrr` (required run rate)

Categoricals are oneâ€‘hot encoded; numerics are passed through in a single scikitâ€‘learn `Pipeline`.

## ğŸ§  Modeling Approach

The training pipeline evaluates multiple classifiers and automatically selects the most efficient one by crossâ€‘validated accuracy:

- Logistic Regression (baseline, wellâ€‘calibrated)
- Random Forest (nonâ€‘linear interactions, tabular strength)
- Gradient Boosting (additive trees, competitive on structured data)

Selection uses 5â€‘fold cross validation on the same preprocessed features. The best mean CV accuracy model is refit on a train split and evaluated on a heldâ€‘out test split. The final pipeline is saved as `pipe.pkl` and consumed directly by `app.py`.

To see the chosen model and its accuracy, run the training script; it prints perâ€‘model CV scores and the final holdâ€‘out accuracy.

## What Model We Use and Why

This project does model selection automatically. During training, we evaluate three scikitâ€‘learn classifiers on identical features and preprocessing:

- Logistic Regression
- Random Forest
- Gradient Boosting

We pick the model with the best crossâ€‘validated accuracy and then wrap it in a single `Pipeline` with preprocessing and apply probability calibration using `CalibratedClassifierCV` (isotonic). The result is saved as `pipe.pkl`.

Why this setup:

- Wellâ€‘calibrated probabilities: Win probability must be numerically meaningful, not just ranked; isotonic calibration improves probability quality.
- Strong tabular performance: Treeâ€‘based models (Random Forest / Gradient Boosting) capture nonâ€‘linear interactions; Logistic Regression provides a strong, simple baseline.
- Robust preprocessing: `OneHotEncoder(handle_unknown='ignore')` makes the model resilient to unseen teams/cities.
- Fast inference: The final calibrated pipeline is lightweight enough for realâ€‘time Streamlit use.
- Transparent and reproducible: Crossâ€‘validated model choice is logged; the app shows the active classifier under â€œModel infoâ€.

Note: The exact chosen classifier can differ across datasets/runs (depending on data and random seeds). Check the training output or the appâ€™s â€œModel infoâ€ expander to see which classifier is currently deployed and whether calibration is active.

## â–¶ï¸ Quick Start

Install dependencies:
```bash
pip install -r requirements.txt
```

Train the model from real data:
```bash
python create_mock_model.py               # full training
# or a faster, lighter run on low-spec machines
python create_mock_model.py --quick       # downsample + lighter CV/params
```

Run the app:
```bash
streamlit run app.py
```

The app runs at `http://localhost:8501`.

### Windows one-liners (CMD and PowerShell)

Command Prompt (CMD):
```bat
cd /d C:\Users\swara\OneDrive\ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ\achine-Learning-powered-IPL-match-win-probability-predictor-with-Streamlit-web-app-main
python create_mock_model.py
streamlit run app.py
```

PowerShell:
```powershell
Set-Location "C:\Users\swara\OneDrive\ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ\achine-Learning-powered-IPL-match-win-probability-predictor-with-Streamlit-web-app-main"
python create_mock_model.py
streamlit run app.py
```

## ğŸ“¦ Project Structure

```
ipl-win-probability-predictor/
â”œâ”€â”€ app.py                   # Streamlit app
â”œâ”€â”€ create_mock_model.py     # Training script (real data + model selection)
â”œâ”€â”€ matches.csv              # Match metadata
â”œâ”€â”€ deliveries.csv           # Ball-by-ball data
â”œâ”€â”€ pipe.pkl                 # Saved model pipeline
â”œâ”€â”€ requirements.txt         # Dependencies
â”œâ”€â”€ Procfile                 # (Optional) Deployment config
â”œâ”€â”€ setup.sh                 # (Optional) Streamlit config
â””â”€â”€ README.md                # Project documentation
```

## ğŸ” Reproducible Metrics

The training script reports:
- Perâ€‘model 5â€‘fold CV accuracy (mean Â± std)
- Chosen model name
- Holdâ€‘out test accuracy

Reâ€‘run the script after updating data or parameters to regenerate metrics.

## ğŸ“ License

MIT License.
